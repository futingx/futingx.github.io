---
layout: post
title:  "YOLOv11：搭建可视化目标检测系统"
categories: YOLOv11：搭建可视化目标检测系统
tags:  YOLOv11：搭建可视化目标检测系统
author: FTX
---

* content
{:toc}

# 跟着工程师学 YOLOv11：从 0 到 1 搭建可视化目标检测系统（附新手避坑指南）




大家好！最近在实习中，跟着公司工程师完整走了一遍 YOLOv11 目标检测项目的开发流程 —— 从环境搭建到可视化界面，再到模型训练和功能优化，踩了不少坑，也积累了很多实用经验。今天把这些整理成一篇保姆级教程，新手跟着做就能上手 YOLOv11，甚至能做出带 GUI 的检测系统，亲测有效！

## 写在前面：为什么选 YOLOv11？

YOLO 系列一直是目标检测领域的 “性价比之王”，而 YOLOv11 作为最新版本，相比前代在**速度（提升 15%）** 和**小目标检测精度（提升 20%）** 上都有优化，还支持轻量化模型（比如 yolo11n.pt 仅 2.5MB），普通电脑也能跑。这次项目我们不仅实现了基础检测，还加了 PySide6 可视化界面，最终做出 “图像检测 + 视频检测 + 模型切换” 的完整系统，新手也能轻松用起来～

## 一、基础准备：YOLOv11 环境搭建（避坑重点！）

环境配置是新手最容易卡壳的地方，工程师当时带着我们用 Anaconda 隔离环境，步骤清晰，几乎没踩坑，大家按这个来：

### 1. 新建并激活虚拟环境

首先用 Anaconda 创建专门的虚拟环境（叫`yolov11`），避免和其他项目依赖冲突：

```bash
# 1. 创建虚拟环境（Python版本固定3.10，兼容性最好）
conda create -n yolov11 python=3.10

# 出现“Proceed ([y]/n)?”时输入y，等待安装

# 2. 激活环境（激活后终端前会显示(yolov11)，代表成功）
conda activate yolov11
```

### 2. 安装 Torch（CPU/GPU 版本任选）

Torch 是 YOLOv11 的核心依赖，工程师推荐新手先装 CPU 版本（体积小、不用配 CUDA，足够练手）：

#### 第一步：先换清华源（加速下载，避免超时）

```bash
# 设置pip清华源
pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple

# 更新pip到最新版
python -m pip install --upgrade pip
```

#### 第二步：安装 Torch

**CPU 版本（推荐新手）**：

```bash
pip3 install torch torchvision torchaudio
```

**GPU 版本（有英伟达显卡 + CUDA≥11.8）**：

先在终端输`nvidia-smi`看 CUDA 版本，然后去[PyTorch 官网](https://pytorch.org/)复制对应命令（比如 CUDA11.8 的命令），记得挂 VPN（不然下载 2.7G 的安装包会很慢）。

### 3. VSCode 配置（工程师手把手教的细节）

安装 VSCode 后，这些配置能少走很多弯路：

1. **装必备插件**：

    打开 VSCode → 扩展面板 → 搜索安装：

    - Python（微软官方插件，语法提示）
    - Qt for Python（和 PySide6 配合做界面）
    - 中文语言包（新手友好，安装后重启生效）

2. **打开项目文件夹**：

    点击 “文件”→“打开文件夹”，选择你的 YOLOv11 项目文件夹（比如我是`ultralytics-8.3.86`）。

3. **改终端为 Command Prompt**：

    默认的 PowerShell 容易出权限问题，工程师让我们改成 CMD：

    打开终端 → 右上角下拉箭头 → “默认配置文件” → 选择 “Command Prompt” → 重启终端。

4. **切换 Python 解释器**：

    点击 VSCode 右下角 “选择解释器” → 找到`yolov11`环境的 Python（路径一般是`Anaconda安装目录\envs\yolov11\python.exe`），选完后终端会显示`(yolov11)`，代表环境生效。

### 4. 安装 Ultralytics 依赖（YOLOv11 核心库）

激活`yolov11`环境后，安装项目依赖，这里有个关键细节 —— 末尾的点不能漏！

```bash
# 安装Ultralytics工程依赖（-e后面的点表示本地安装）
pip install -e .
```

安装完测试一下，跑官方的示例检测，能出结果就代表环境没问题：

```bash
# 用基础模型yolo11n.pt检测自带样本
yolo predict model=yolo11n.pt source=ultralytics\assets
```

第一次跑会下载`yolo11n.pt`模型，检测结果会保存在`runs/detect/predict`文件夹里。

## 二、PySide6 可视化界面：让检测更直观（工程师带做的 GUI）

光用命令行检测不够直观，工程师教我们用 PySide6 做个可视化界面，拖拖拽拽就能搞定，步骤如下：

### 1. 安装 PySide6 并找到 Designer

先在`yolov11`环境里装 PySide6：

```bash
pip install pyside6
```

然后找到可视化界面设计工具`designer.exe`，路径一般是：

`C:\用户\你的用户名\miniconda3\envs\yolov11\Lib\site-packages\PySide6\designer.exe`

（找不到的话在 Anaconda 安装目录里搜 “designer.exe”）

### 2. 用 Designer 画界面（拖控件就行）

工程师让我们按这个流程设计界面，新手也能快速上手：

1. 双击`designer.exe`打开 → 新建界面，选择 “Main Window” → 点击 “创建”。
2. 拖入这些控件（左侧 “窗口部件盒” 里找）：
    - 2 个`QLabel`（用于显示 “原始图像” 和 “检测结果图像”，背景设为白色，方便看图像）。
    - 2 个`QPushButton`（分别命名 “图像检测”“视频检测”，用于触发功能）。
    - 1 个`QComboBox`（下拉框，用于切换 “基础模型” 和 “自训练模型”）。
    - 1 个`Horizontal Slider`（水平进度条，用于视频检测时显示进度）。
3. 布局调整：
    - 先把 2 个`QLabel`水平布局（选中它们，右键→“布局”→“水平布局”）。
    - 再把 “水平布局的标签”“进度条”“两个按钮”“下拉框” 垂直布局（右键→“布局”→“垂直布局”），这样界面会自适应大小。
4. 保存 UI 文件：

    存到项目目录（比如`ultralytics-8.3.86`），命名为`main_window.ui`（**别用中文！别用中文！别用中文！** 工程师强调了三遍）。

### 3. 把 UI 文件转成 Python 代码

Designer 画的`.ui`文件不能直接用，需要转成 Python 代码：

1. 打开 VSCode 终端，激活`yolov11`环境，cd 到`main_window.ui`所在目录。
2. 执行转换命令：

```bash
pyside6-uic main_window.ui -o main_window_ui.py
```

也可以右键`main_window.ui` → 选择 “Compile Qt file”，自动生成代码，更方便。

3. 生成的`main_window_ui.py`里有界面的类`Ui_MainWindow`，后续在业务代码里导入就能用。

### 4. 信号与槽：让按钮 “有反应”

这是 PySide6 的核心机制 —— 按钮点击（信号）触发函数（槽）。工程师用 “图像检测按钮” 举例，教我们怎么绑定：

比如在`base.py`里，把 “图像检测按钮” 的`clicked`信号和`get_image_path`函数（负责打开图像文件）绑定：

```python
# 信号与槽绑定（按钮点击→调用get_image_path函数）
self.pushButton.clicked.connect(self.get_image_path)

# 图像检测的核心函数（槽函数）
def get_image_path(self):
    # 打开图像文件选择框
    path = QFileDialog.getOpenFileName(filter='*.png;*.jpg;*.bmp')
    image_path = path[0]  # 获取选中的图像路径
    if image_path:
        # 调用YOLOv11检测图像
        result = self.model(source=image_path)
        # 显示原始图像和检测结果（后续讲显示逻辑）
        self.show_image(result[0].orig_img, self.label_orig)
        self.show_image(result[0].plot(), self.label_det)
```

简单说：只要点 “图像检测” 按钮，就会触发`get_image_path`函数，打开文件选择框让你选图，然后自动检测并显示结果。

## 三、数据集准备：给 YOLOv11 “喂饭”（打标签 + 文件夹配置）

模型训练需要数据，工程师带我们从视频抽帧到打标签，一步步做好数据准备：

### 1. 新建数据集文件夹结构

先在项目根目录建`datasets`文件夹，里面再建 2 个文件夹：

```
datasets/
├─ images/  # 放抽帧后的图像
│  ├─ train/  # 训练集图像（80%数据）
│  └─ val/    # 验证集图像（20%数据）
└─ labels/  # 放图像对应的标签
   ├─ train/  # 训练集标签（和images/train对应）
   └─ val/    # 验证集标签（和images/val对应）
```

**重点**：`images`和`labels`里的`train`、`val`必须一一对应，比如`images/train/1.jpg`的标签是`labels/train/1.txt`，YOLOv11 训练时会按这个规矩找数据。

### 2. 视频抽帧：用 OpenCV 生成图像

如果没有现成的图像，可以从视频里抽帧。工程师写了个`pic.py`脚本（放`datasets`目录下），专门用来抽帧：

```python
import cv2

# 打开视频文件（替换成你的视频路径）
video = cv2.VideoCapture('datasets/BVN.mp4')

num = 0  # 帧计数器
interval = 30  # 抽帧间隔（每30帧抽1帧，避免数据冗余）
save_num = 1  # 保存的图像编号

while True:
    ret, frame = video.read()  # 读取一帧
    if not ret:  # 视频读完了，退出循环
        break
    num += 1
    # 每隔interval帧保存一次图像
    if num % interval == 0:
        cv2.imwrite(f'datasets/images/train/{save_num}.jpg', frame)
        save_num += 1

print(f'抽帧完成！共生成{save_num-1}张图像')
```

运行脚本：`python datasets/pic.py`，图像会自动保存到`images/train`里。

### 3. 给图像打标签：用 makesense.ai（免费在线工具）

工程师推荐用[makesense.ai](https://www.makesense.ai/)打标签，不用装软件，新手友好：

1. 打开网站 → 点击 “Get Started” → 上传`images/train`里的图像。
2. 新建标签：点击 “Labels”→“Add Label”，输入标签名（**必须是英文！** 比如 “daitu”“mingren”）。
3. 打标签：选择图像 → 用 “Rectangle” 工具框选目标 → 选择对应标签 → 保存。
4. 导出标签：所有图像打完后，点击 “Export Annotations”→ 选择 “YOLO” 格式 → 导出压缩包。
5. 解压标签：把导出的`labels`文件夹里的`.txt`文件，放到`datasets/labels/train`里；再复制一份`images/train`和`labels/train`，改名为`val`，作为验证集。

### 4. 修改.yaml 配置文件

新建一个`bvn.yaml`文件（放项目根目录），告诉 YOLOv11 数据集在哪、有哪些类别：

```yaml
# 训练集和验证集路径（根据你的文件夹路径调整）
train: ../datasets/images/train
val: ../datasets/images/val

# 类别数和类别名（和你打标签的名字一致）
nc: 2
names: ['daitu', 'mingren']
```

## 四、模型训练：让 YOLOv11 “学会” 检测（避坑指南）

数据准备好后，就能训练模型了，工程师帮我们避开了几个常见坑：

### 1. 先做 2 个关键准备（不然训练会报错）

#### 坑 1：虚拟内存不足

训练时如果报 “内存不足”，工程师教我们调大虚拟内存：

- 右键 “此电脑”→“属性”→“高级系统设置”→“高级”→“性能”→“设置”→“高级”→“虚拟内存”→“更改”。
- 选择项目所在磁盘（比如 D 盘）→ 勾选 “自定义大小”→ 初始大小 1024MB，最大值设为 100000MB（几万就行）→ 点击 “设置”→“确定”，重启电脑生效。

#### 坑 2：字体下载失败

训练时会生成可视化图表（比如混淆矩阵），默认下载字体容易失败，工程师让我们手动加字体：

- 找到`C:\Users\你的用户名\AppData\Roaming\Ultralytics`文件夹，删除里面所有文件。
- 把 Arial.ttf 字体文件（网上能搜到，或找工程师要）放到这个文件夹里，解决字体问题。

### 2. 写 train.py 脚本并运行

新建`train.py`（放项目根目录），工程师帮我们写了简化版脚本，新手改改路径就能用：

```python
from ultralytics import YOLO

# 加载基础模型（yolo11n.pt）
model = YOLO('yolo11n.pt')

# 训练模型（改这里的参数！）
model.train(
    data='bvn.yaml',  # 你的数据集配置文件
    epochs=50,        # 训练轮次（50轮足够新手练手）
    workers=0         # 数据加载线程（Windows设0，避免报错）
)

# 训练完用自训练模型检测（可选）
# model = YOLO('runs/detect/train/weights/best.pt')
# model(source='datasets/BVN.mp4', show=True, save=True)
```

运行脚本：`python train.py`，第一次跑会慢一点，耐心等。训练结果会保存在`runs/detect/train`里，其中：

- `best.pt`：验证集精度最高的模型（推荐用这个）。
- `last.pt`：最后一轮训练的模型（训练中断后可用来恢复）。

## 五、进阶功能：让系统更实用（工程师带做的优化）

基础检测做好后，工程师还教我们加了几个实用功能，提升体验：

### 1. 模型切换：用下拉框选基础 / 自训练模型

在`base.py`里，通过`QComboBox`实现模型切换，核心是`model_changed`函数：

```python
# 初始化下拉框（添加模型选项）
def model_init(self):
    self.comboBox.addItem("基础模型")  # 对应yolo11n.pt
    self.comboBox.addItem("自训练模型")  # 对应best.pt
    # 下拉框选中项变化时，触发model_changed函数
    self.comboBox.currentIndexChanged.connect(self.model_changed)

# 模型切换的核心函数
def model_changed(self):
    index = self.comboBox.currentIndex()
    if index == 0:
        # 加载基础模型
        self.model = YOLO('yolo11n.pt')
    elif index == 1:
        # 加载自训练模型（路径要对）
        self.model = YOLO('runs/detect/train/weights/best.pt')
    print(f'切换到：{self.comboBox.currentText()}')
```

这样在界面上选下拉框，就能切换模型，不用改代码。

### 2. 视频检测：用定时器实现实时播放

视频检测需要逐帧处理，工程师教我们用`QTimer`（定时器）控制帧率，核心代码在`base.py`的`video_init`和`video_detect`函数：

```python
# 视频检测初始化（绑定按钮和定时器）
def video_init(self):
    # 视频检测按钮绑定函数
    self.pushButton_video.clicked.connect(self.get_video_path)
    # 定时器（30ms触发一次，对应约33FPS，播放流畅）
    self.timer = QTimer(self)
    self.timer.setInterval(30)
    # 定时器触发时，调用video_detect函数（处理一帧）
    self.timer.timeout.connect(self.video_detect)

# 视频检测核心函数（逐帧处理）
def video_detect(self):
    ret, frame = self.video.read()  # 读取一帧
    if ret:  # 有帧可读，进行检测
        result = self.model(source=frame)
        # 显示原始帧和检测帧
        self.show_image(result[0].orig_img, self.label_orig)
        self.show_image(result[0].plot(), self.label_det)
    else:  # 视频读完，停止定时器
        self.timer.stop()
        print("视频检测完毕！")
```

### 3. 2 个关键优化（解决新手常遇问题）

#### 优化 1：视频检测加进度条

在 Designer 里拖入`Horizontal Slider`控件后，在`base.py`里加进度条逻辑：

```python
# 视频加载时，设置进度条最大值（总帧数）
def get_video_path(self):
    path = QFileDialog.getOpenFileName(filter='*.mp4;*.avi')
    video_path = path[0]
    if video_path:
        self.video = cv2.VideoCapture(video_path)
        # 获取视频总帧数，设为进度条最大值
        total_frames = int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))
        self.horizontalSlider.setMaximum(total_frames)
        self.timer.start()

# 每帧检测时，更新进度条位置
def video_detect(self):
    ret, frame = self.video.read()
    if ret:
        # ... 检测逻辑 ...
        # 更新进度条（当前帧数）
        current_frame = int(self.video.get(cv2.CAP_PROP_POS_FRAMES))
        self.horizontalSlider.setValue(current_frame)
```

这样视频播放时，进度条会实时动，拖拽进度条还能跳帧（需要加`sliderReleased`函数，参考前面的信号与槽）。

#### 优化 2：修复视频 / 图像检测冲突

新手常遇到 “视频检测时点击图像检测没反应”，因为视频定时器还在跑，资源没释放。工程师教我们写个通用函数`stop_video_detection`，在切换检测模式前调用：

```python
# 通用函数：停止视频检测并释放资源
def stop_video_detection(self):
    if self.timer.isActive():
        self.timer.stop()  # 停止定时器
    if self.video.isOpened():
        self.video.release()  # 释放视频资源
    self.label_orig.clear()  # 清空图像显示
    self.label_det.clear()
    self.horizontalSlider.setValue(0)  # 重置进度条

# 图像检测按钮触发时，先停止视频检测
def get_image_path(self):
    self.stop_video_detection()  # 关键：先停视频
    # ... 后续图像检测逻辑 ...
```

这样点图像检测时，会先停掉视频，再也不会冲突了。

## 六、实习心得：跟着工程师学到的 3 个关键点

1. **环境隔离很重要**：一开始我直接用 base 环境装依赖，结果各种版本冲突，工程师教我用虚拟环境后，再也没出现过 “装 A 卸 B” 的问题。
2. **数据是训练的灵魂**：刚开始我随便找了几张图训练，模型精度很低，工程师提醒我 “数据要多样（不同角度、光照）、标签要准”，调整后精度从 70% 升到 90%+。
3. **先跑通再优化**：新手别一开始就追求复杂功能，先把 “环境→检测→训练” 跑通，再加界面、优化功能，不然很容易放弃。

## 最后：项目扩展方向

如果想继续深化这个项目，工程师给了几个方向：

- 加 “实时摄像头检测”：用`cv2.VideoCapture(0)`调用电脑摄像头。
- 加 “检测结果保存”：在检测函数里加`cv2.imwrite`保存图像，`cv2.VideoWriter`保存视频。
- 模型轻量化：试试更小号的模型`yolo11-pico.pt`，速度更快，适合部署到手机 / 嵌入式设备。

希望这篇博客能帮到想入门 YOLOv11 的朋友，跟着步骤走，你也能做出自己的可视化检测系统～ 有问题欢迎在评论区交流！
